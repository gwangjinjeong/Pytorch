{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Softmax_classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPKtmgI7AW9Lzj+p//nkuJb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gwangjinjeong/Pytorch/blob/master/basic/04_Softmax_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMjtxYaQV_YN",
        "colab_type": "text"
      },
      "source": [
        "# Softmasx Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AYleATYYt4A",
        "colab_type": "text"
      },
      "source": [
        "* What is Softmax\n",
        "* Cross Entropy\n",
        "* Low-level Implementation\n",
        "* High-level Implementation\n",
        "* Training Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjgFxHJrY_ai",
        "colab_type": "text"
      },
      "source": [
        "# *0. Preparation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9X9otNNWEGL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be83e66e-4aeb-4ae1-a4a1-bd83b69d3c1a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ffa1ccb6df0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc-4nRRfZMe2",
        "colab_type": "text"
      },
      "source": [
        "+) Discrete Probability distribution (이산확률분포)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5R4tZCofN5U",
        "colab_type": "text"
      },
      "source": [
        "조건부 확률   \n",
        "P(주먹|가위) => 가위를 냈을때, 주먹을 낼 확률값.   \n",
        "P(가위|가위) => 가위를 냈을때, 가위을 낼 확률값.   \n",
        "P(보|가위) => 가위를 냈을때, 보을 낼 확률값.   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tznVbDeDfe77",
        "colab_type": "text"
      },
      "source": [
        "# *1. Softmax*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzx8S_AEfhwd",
        "colab_type": "text"
      },
      "source": [
        "max값을 soft하게 뽑아줘"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZWn6AaIZLsQ",
        "colab_type": "text"
      },
      "source": [
        "$$ P(class=i) = \\frac{e^i}{\\sum e^i} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0MhVOQ0aiB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = torch.FloatTensor([1, 2, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0LfNcmbfl2D",
        "colab_type": "text"
      },
      "source": [
        "여기서 amx를 뽑는다면 (0, 0, 1)으로 나오게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z90cXWGPaiCB",
        "colab_type": "text"
      },
      "source": [
        "하지만 `softmax` 는,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD0cGgjgaiCC",
        "colab_type": "code",
        "outputId": "069b2ca4-60c5-4296-db3b-14a66cfa269f",
        "colab": {}
      },
      "source": [
        "hypothesis = F.softmax(z, dim=0)\n",
        "print(hypothesis)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48xkeiwCft_e",
        "colab_type": "text"
      },
      "source": [
        "위와 같이 `마지막 3의 값이 66%로 max값이다.`와 같이 확률값으로 나타나게 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtYL1dFNaiCI",
        "colab_type": "code",
        "outputId": "1117f8f5-b674-47be-8afe-83eddc178248",
        "colab": {}
      },
      "source": [
        "hypothesis.sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHMo-NCphH71",
        "colab_type": "text"
      },
      "source": [
        "# *2. Cross Entropy*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F96xweTohK3y",
        "colab_type": "text"
      },
      "source": [
        "$ H(P, Q) = -E_{x\\sim P(x)}[ logQ(x) ]$ = $- \\sum_{x\\in X}P(x)logQ(x)$   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpdEKuqchHdT",
        "colab_type": "text"
      },
      "source": [
        "가위바위보 게임에서, $Q_1$이 바위, $Q_2$가 보라고 가정하고, \n",
        "상대방이 가위를 낼 확률이 P라고 했을때, 상대방를 이기기 위해서\n",
        "H(P, Q)를 최소화 하도록 한다면, 점점 Q가 P에 가까워질 것이므로, 어떤 값을 낼지 예측할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDoeBQSxaiCM",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Cross Entropy Loss (Low-level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Y7TITuaiCN",
        "colab_type": "text"
      },
      "source": [
        "다중 분류를 하는데 사용되는 loss를 Cross Entropy loss라고 한다.   \n",
        "어떻게 최소화 하는가?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnP9ClZ-aiCO",
        "colab_type": "text"
      },
      "source": [
        "$ L = \\frac{1}{N} \\sum - y \\log(\\hat{y}) $   \n",
        "y는 실제 값, P(X)을 의미한다.   \n",
        "$P_\\theta$ $\\theta$를 낼 확률을 의미한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZkAX01lVGp",
        "colab_type": "text"
      },
      "source": [
        "예)\n",
        "클래스 갯수를 5개   \n",
        "sample 갯수를 3개로 만들어보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4xo7CBZyaiCR",
        "colab_type": "code",
        "outputId": "5fbeac8e-ac08-4aef-e460-dfcbd23060b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "z = torch.rand(3, 5, requires_grad=True)\n",
        "hypothesis = F.softmax(z, dim=1) # 즉, 두번째 dimantion이 결과값이다. 즉, y^ d이다.\n",
        "print(hypothesis)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
            "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
            "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHT8EAxvaiCU",
        "colab_type": "code",
        "outputId": "4a6daed4-ec9c-4e83-cc4b-481fda73f197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y = torch.randint(5, (3,)).long()\n",
        "print(y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_ujbroZaiCY",
        "colab_type": "code",
        "outputId": "333d4efb-9c5d-41cc-c623-631a7e1e2b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "y_one_hot = torch.zeros_like(hypothesis) # (3,5)를 만들어줘라.\n",
        "y_one_hot.scatter_(1, y.unsqueeze(1), 1) # dim 1에 대해서 unsqueeze를 해서 1을 뿌려줘라.\n",
        "# y의 size는 (3,) => y.unsqueeze는 (3,1)로 되었다."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcaLUeYiaiCb",
        "colab_type": "code",
        "outputId": "ed90dfeb-6436-4d75-8119-9ae8efb6f087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print((y_one_hot * -torch.log(hypothesis)))\n",
        "print((y_one_hot * -torch.log(hypothesis)).sum(dim=1))\n",
        "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
        "print(cost)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.3301, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 1.4602, 0.0000, 0.0000],\n",
            "        [0.0000, 1.6165, 0.0000, 0.0000, 0.0000]], grad_fn=<MulBackward0>)\n",
            "tensor([1.3301, 1.4602, 1.6165], grad_fn=<SumBackward1>)\n",
            "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4qI2O0EaiCe",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Cross-entropy Loss with `torch.nn.functional`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xFr067vaiCf",
        "colab_type": "text"
      },
      "source": [
        "PyTorch has `F.log_softmax()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unM04QIOaiCg",
        "colab_type": "code",
        "outputId": "6b5eeb07-2fad-4d46-ab5f-819f1f3b7a05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Low level\n",
        "torch.log(F.softmax(z, dim=1))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
              "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
              "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRxA4PsRaiCj",
        "colab_type": "code",
        "outputId": "b0a70e91-da13-4470-ae7f-1054481b05bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# High level\n",
        "F.log_softmax(z, dim=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
              "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
              "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
              "       grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tq58DTjaiCm",
        "colab_type": "text"
      },
      "source": [
        "PyTorch also has `F.nll_loss()` function that computes the negative loss likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0HTX7q_aiCn",
        "colab_type": "code",
        "outputId": "d7d6c0b6-9c54-45ad-cd02-6cd88d182dc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Low level\n",
        "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlFHkfhlaiCr",
        "colab_type": "code",
        "outputId": "b695eb32-35df-4366-baa6-c5cfc3c1e8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# High level NLL = Negative Log Likelihood\n",
        "F.nll_loss(F.log_softmax(z, dim=1), y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48fjmSBXaiCv",
        "colab_type": "text"
      },
      "source": [
        "PyTorch also has `F.cross_entropy` that combines `F.log_softmax()` and `F.nll_loss()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU547oFWaiCw",
        "colab_type": "code",
        "outputId": "32c3b205-70f8-4a84-8892-723ed486239f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# nll_loss + log_softmax\n",
        "F.cross_entropy(z, y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoy_Oq6NmuuE",
        "colab_type": "text"
      },
      "source": [
        "일반적으로 확률 값을 봐야하는 경우가 많기 때문에,   \n",
        "Neural network에서는 softmax 이전 값을 가져와야하기도 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNCPqNZQaiCz",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 High-level Implementation with\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t-XMV4onLyW",
        "colab_type": "text"
      },
      "source": [
        "Make the Train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZEKPxEraiC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = [[1, 2, 1, 1],\n",
        "           [2, 1, 3, 2],\n",
        "           [3, 1, 3, 4],\n",
        "           [4, 1, 5, 5],\n",
        "           [1, 7, 5, 5],\n",
        "           [1, 2, 5, 6],\n",
        "           [1, 6, 6, 6],\n",
        "           [1, 7, 7, 7]]\n",
        "y_train = [2, 2, 2, 1, 1, 1, 0, 0] # Discrete 한 값. \n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o8pzY1iaiC4",
        "colab_type": "code",
        "outputId": "6f80bc4b-1b3c-4fb1-9c01-b2f1ac3726ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# 모델 초기화\n",
        "# vector size(columns)  = 4\n",
        "# number of sample(row) = m\n",
        "# classes               = 3\n",
        "W = torch.zeros((4, 3), requires_grad=True) \n",
        "b = torch.zeros(1, requires_grad=True)      \n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=0.1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # Cost 계산 (1)\n",
        "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) # or .mm or @\n",
        "    y_one_hot = torch.zeros_like(hypothesis)\n",
        "    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
        "    cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n",
        "    # 물론 위 3개를     cost = F.cross_entropy(z, y_train)로 만들어줄 수도 있다.\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 1.098612\n",
            "Epoch  100/1000 Cost: 0.901535\n",
            "Epoch  200/1000 Cost: 0.839114\n",
            "Epoch  300/1000 Cost: 0.807826\n",
            "Epoch  400/1000 Cost: 0.788472\n",
            "Epoch  500/1000 Cost: 0.774822\n",
            "Epoch  600/1000 Cost: 0.764449\n",
            "Epoch  700/1000 Cost: 0.756191\n",
            "Epoch  800/1000 Cost: 0.749398\n",
            "Epoch  900/1000 Cost: 0.743671\n",
            "Epoch 1000/1000 Cost: 0.738749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-LrZT2oaiC-",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 High-level Implementation with `nn.Module`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZzsdWmFaiC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifierModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(4, 3) # Output이 3(class 갯수)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x) # |x| = (m,4) => (m,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGmBbXqsaiDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SoftmaxClassifierModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQHN2qb2aiDE",
        "colab_type": "text"
      },
      "source": [
        "Let's try another new dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3lcullRaiDF",
        "colab_type": "code",
        "outputId": "3967a8f1-f735-44ad-f7e7-b85e8c7aa297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# optimizer 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train) # |x_train| = (m,4), |prediction| = (m,3) \n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.cross_entropy(prediction, y_train) #|y_train | = (m,)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # 20번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 1.849513\n",
            "Epoch  100/1000 Cost: 0.689894\n",
            "Epoch  200/1000 Cost: 0.609258\n",
            "Epoch  300/1000 Cost: 0.551218\n",
            "Epoch  400/1000 Cost: 0.500141\n",
            "Epoch  500/1000 Cost: 0.451947\n",
            "Epoch  600/1000 Cost: 0.405051\n",
            "Epoch  700/1000 Cost: 0.358733\n",
            "Epoch  800/1000 Cost: 0.312912\n",
            "Epoch  900/1000 Cost: 0.269522\n",
            "Epoch 1000/1000 Cost: 0.241922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wpRUUGWoqmm",
        "colab_type": "text"
      },
      "source": [
        "**1과 0으로 이루어진 binary classification(이진분류)**   \n",
        "binary classification는 logistic regression과 굉장히 유사한데, 그래서 binary classification에서는 binary corss entorpy loss를 사용하는것이 맞다.\n",
        "그리고 sigmoid를 써야한다.      \n",
        "\n",
        "**2개 이상의 multi-class로 이루어진 분류**   \n",
        "이때는, Cross-entropy를 사용하고, softmax를 사용해야한다.\n"
      ]
    }
  ]
}